{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 1 - Intro to Language Models",
      "provenance": [],
      "collapsed_sections": [
        "USUR5P5QNMj0",
        "Yq5svZeqa2jB",
        "6dFOYlsFa2jB",
        "cicK6Oa2a2jC",
        "KlFCwMHle_cl",
        "uMxOhWZ8a2jD",
        "B4BiS4Lda2jE",
        "WNFc6zbVa2jF",
        "nlB32jo6N18Y",
        "NTtMqXcAhj-Z",
        "xFN4g2WHZjYD",
        "bSasXHkbFht2",
        "fW_-stCIiJAf",
        "Zl-Ti1-DbYIS",
        "dJBfYUx7bYIT",
        "V1mAiegcbYIU",
        "5ZI6BiOgje0L",
        "AR1rvF-uchGf",
        "l0VVP0ftbYIX",
        "sfvNK-7rkVHQ",
        "bjD2VS_vbYIY",
        "J0MqsfE9sa2Z",
        "-c9QbpX8wXpu",
        "3BS4y5ZXwXp3",
        "vYd758lKuWpV",
        "OHqKaYnbd07_",
        "4cTC7-8vPT8T",
        "BeR-qb63d8IV"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanessa920/2021-ComputeFest/blob/main/Lab_1_Intro_to_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7_3NVx0NEaC"
      },
      "source": [
        "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
        "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> <a href='https://www.computefest.seas.harvard.edu/' target='_blank'><strong>IACS: ComputeFest 2021</strong></a></h1>\n",
        "\n",
        "# Language Models\n",
        "\n",
        "#### **Authors/Instructors:**\n",
        "Chris Tanner, Shivas Jayaram, Rohit Beri, Zhao Lyu, Xiaohan Yang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USUR5P5QNMj0"
      },
      "source": [
        "## <font color=\"darkred\">Workshop Outline</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng4VXk0VNTB2"
      },
      "source": [
        "1. [**Setup Notebook**](#Setup-Notebook)\n",
        "2. [**Text Generation**](#Text-Generation)\n",
        " - ***Transformers***\n",
        " - ***GPT-2 Pretrained Lanaguage Model***\n",
        "    - Overview\n",
        "    - Greedy Search\n",
        "    - Beam Search\n",
        "    - Top-K Sampling\n",
        "    - Top-p Sampling\n",
        "    - Sampling using Temperature\n",
        "    - Interactive Examples\n",
        "    - Parameters for text generation\n",
        " - ***Additional Notes***\n",
        "3. [**References**](#References)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwE0QMQraw61"
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML, display\n",
        "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
        "\n",
        "def style():\n",
        "    HTML(styles)\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', style)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFB2jiS7g-AW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq5svZeqa2jB"
      },
      "source": [
        "## <font color=\"darkred\">Setup Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dFOYlsFa2jB"
      },
      "source": [
        "### <font color=\"green\">Copy & Setup Colab with GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVyL4foca2jC"
      },
      "source": [
        "1) Select \"File\" menu and pick \"Save a copy in Drive\"  \n",
        "2) This notebooks is already setup to use GPU but if you want to change it. Go to \"Runtime\" menu and select \"Change runtime type\". Then in the popup in \"Hardware accelerator\" select \"GPU\" and then click \"Save\"   \n",
        "3) If you want high RAM there is an option for that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cicK6Oa2a2jC"
      },
      "source": [
        "### <font color=\"green\">Installs\n",
        "\n",
        "Install ```transformers``` library (State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.) from https://huggingface.co/transformers/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F3lUYIEa2jC"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlFCwMHle_cl"
      },
      "source": [
        "### <font color=\"orange\">Nano Quiz</font>\n",
        "\n",
        "**1.** What is the HuggingFace?\n",
        "* A.  a library that contains many language models\n",
        "* B.  a specific type of Transformers\n",
        "* C.  an emoji used frequently in NLP community\n",
        "* D.  an NLP-focused startup with a large open-source community, in particular around the Transformers library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDcSHQT0fB4v"
      },
      "source": [
        "#### <font color=\"purple\">Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMtXavVFfRxQ"
      },
      "source": [
        "D.  an NLP-focused startup with a large open-source community, in particular around the Transformers library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMxOhWZ8a2jD"
      },
      "source": [
        "### <font color=\"green\">Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htc9HyPea2jE"
      },
      "source": [
        "import sys\n",
        "import logging\n",
        "from argparse import ArgumentParser\n",
        "from subprocess import call\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4BiS4Lda2jE"
      },
      "source": [
        "### <font color=\"green\">Setup Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6PwH80Xa2jE"
      },
      "source": [
        "# Setup Logger\n",
        "if '__file__' not in globals():\n",
        "  __file__ = \".\"\n",
        "logger = logging.getLogger(__file__)\n",
        "\n",
        "# Logger config\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNFc6zbVa2jF"
      },
      "source": [
        "### <font color=\"green\">Verify Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28Ie6FqJa2jF"
      },
      "source": [
        "# Enable/Disable Eager Execution\n",
        "# Reference: https://www.tensorflow.org/guide/eager\n",
        "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
        "# without building graphs\n",
        "\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "#tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "logger.info('__Python VERSION: %s', sys.version)\n",
        "logger.info(\"tensorflow version: %s\", tf.__version__)\n",
        "logger.info(\"keras version: %s\", tf.keras.__version__)\n",
        "logger.info(\"Eager Execution Enabled: %s\", tf.executing_eagerly())\n",
        "\n",
        "# Get the number of replicas \n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "logger.info(\"Number of replicas: %s\", strategy.num_replicas_in_sync)\n",
        "\n",
        "devices = tf.config.experimental.get_visible_devices()\n",
        "logger.info(\"Devices: %s\", devices)\n",
        "logger.info(tf.config.experimental.list_logical_devices('GPU'))\n",
        "\n",
        "logger.info(\"GPU Available: %s\", tf.config.list_physical_devices('GPU'))\n",
        "logger.info(\"All Pysical Devices: %s\", tf.config.list_physical_devices())\n",
        "\n",
        "# nvidia-smi\n",
        "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
        "\n",
        "# Better performance with the tf.data API\n",
        "# Reference: https://www.tensorflow.org/guide/data_performance\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRt8v3G_Fydv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlB32jo6N18Y"
      },
      "source": [
        "## <font color=\"darkred\">Text Generation using Language Models\n",
        "\n",
        "> **Language Model:**\n",
        ">> A machine learning model that is able to look at part of a sentence and predict the next word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTtMqXcAhj-Z"
      },
      "source": [
        "### <font color=\"green\">Transformers</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig7vbyCQN2ic"
      },
      "source": [
        "* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "<br>\n",
        "\n",
        "A **transformer** is an attention based deep learning model that has proven to be effective in some common NLP tasks. It is essentially a stack of encoder and decoder layers, where the encoder encodes our input using the attention mechanism, and the decoder uses the information encoded in the encoder to give the output. Unlike RNNs/LSTMs, tranformers lend themselves to parallelization.\n",
        "\n",
        "![Transformer](https://miro.medium.com/max/4800/1*zashbcHkygPg0GGEzrmemg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFN4g2WHZjYD"
      },
      "source": [
        "### <font color=\"green\">GPT-2 Pretrained Lanaguage Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSasXHkbFht2"
      },
      "source": [
        "#### <font color=\"orange\">Overview</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oewz3ORoBGk"
      },
      "source": [
        "* [Open AI: GPT-2](https://openai.com/blog/better-language-models/)\n",
        "* [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/)\n",
        "\n",
        "<br>\n",
        "\n",
        "GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains.\n",
        "\n",
        "> GPT-2 (a successor to GPT), was trained simply to predict the next word in 40GB of Internet text. GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. \n",
        "\n",
        "<br>\n",
        "\n",
        "In addition, GPT-2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute.\n",
        "\n",
        "> The GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. One key difference between the two is that GPT2, like traditional language models, outputs one token at a time.\n",
        "\n",
        "<br>\n",
        "\n",
        "![GPT-2](http://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW_-stCIiJAf"
      },
      "source": [
        "#### <font color=\"orange\">Load Model & Tokenizer</font>\n",
        "\n",
        "Let's first initalize the pretrained GPT-2 model and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZftBJqvbYIS"
      },
      "source": [
        "# Tokenizer - Converts text into numerical tokens. \n",
        "# Special tokens for Start/end of text and words unknown to the pretrained model.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Model - Load pretrained GPT Language Generation Model\n",
        "# Lanaguage Generation model generates text based on an input of beginning of a sentence.\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl-Ti1-DbYIS"
      },
      "source": [
        "#### <font color=\"orange\">Input Text</font>\n",
        "Here we use the GPT2 tokenizer to have the input encoded by the encoder of GPT2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eXRtEOAbYIS"
      },
      "source": [
        "tf.random.set_seed(1234)\n",
        "\n",
        "# Tokenize Input\n",
        "input_ids = tokenizer.encode('Today is a great day', return_tensors='tf')\n",
        "print(\"input_ids\",input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJBfYUx7bYIT"
      },
      "source": [
        "#### <font color=\"orange\">Text generation using Greedy Search</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bu1GVDECRZr"
      },
      "source": [
        "Greedy search will simply select the word with the highest probability as its next word: $w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$. \n",
        "\n",
        "![Greedy](https://www.techopedia.com/images/uploads/cdbfcae0113e42b0b02a0821db92d660.PNG)\n",
        "\n",
        "> Below is an example to generate the next five text that are most likely to appear. (The last text is a shifted line)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAyB-gDzbYIT"
      },
      "source": [
        "output_ids = tf.identity(input_ids)\n",
        "for i in range(5):\n",
        "  model_outputs = model(input_ids=output_ids)\n",
        "  next_token_logits = model_outputs[0][:, -1, :]\n",
        "\n",
        "  # Greedy decoding\n",
        "  next_token = tf.math.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
        "\n",
        "  next_token = tf.reshape(next_token, [1,-1])\n",
        "  output_ids = tf.concat([output_ids,next_token], axis=1)\n",
        "  print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K7oDe67bYIT"
      },
      "source": [
        "# Using the prebuilt method in \"transformers\" model\n",
        "# max_length is the maximum length of the whole text, including input words and generated ones.\n",
        "\n",
        "outputs = model.generate(input_ids, max_length=10,num_return_sequences=1)\n",
        "print(\"Generated text:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1mAiegcbYIU"
      },
      "source": [
        "#### <font color=\"orange\">Text generation using Beam search</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLtf2PeOBG-0"
      },
      "source": [
        "One important drawback of greedy search is that it would miss words with high probability but hidden behind words with low probability. We can keep track of multiple words with probabilities higher than others to reduce the risk of missing words with high probability hidden by those with low probability. This is exactly what Beam search does, and it choose the one with the highest probability when it turns to end.\n",
        "\n",
        "Beam search is an improved version of greedy search. It has a hyperparameter named beam size, $k$ . At time step 1, we select to $k$ tokens with the highest conditional probabilities. Each of them will be the first token of $k$ candidate output sequences, respectively. At each subsequent time step, based on the $k$ candidate output sequences at the previous time step, we continue to select $k$ candidate output sequences with the highest conditional probabilities.\n",
        "\n",
        "![Beam Search](https://d2l.ai/_images/beam-search.svg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-KTs32-bYIU"
      },
      "source": [
        "# Beam search\n",
        "n = 2\n",
        "\n",
        "# Predict from model\n",
        "def predict_from_model(model,input_ids):\n",
        "  model_outputs = model(input_ids=input_ids)\n",
        "  next_token_logits = model_outputs[0][:, -1, :]\n",
        "  top_n = tf.math.top_k(next_token_logits, k=n)\n",
        "  top_n_logits = top_n[0]\n",
        "  top_n_scores = tf.nn.softmax(top_n_logits, axis=-1)\n",
        "  top_n_index = top_n[1]\n",
        "  return top_n_index, top_n_scores\n",
        "\n",
        "def generate_beam_search_options(level,score, model, current_output_ids):\n",
        "  top_n_index_next, top_n_scores_next = predict_from_model(model, current_output_ids)\n",
        "  for k in range(n):\n",
        "    next_score = top_n_scores_next[0][k].numpy()\n",
        "    next_next_token = tf.reshape(top_n_index_next[0][k], [1,-1])\n",
        "    next_current_output_ids = tf.concat([current_output_ids,next_next_token], axis=1)\n",
        "    output_text = tokenizer.decode(next_current_output_ids[0], skip_special_tokens=True)\n",
        "    print(\"\\t\"*level,k,output_text,\":\",next_score,\",\", next_score+score)\n",
        "\n",
        "    if level < 3:\n",
        "      generate_beam_search_options(level+1,next_score+score, model, next_current_output_ids)\n",
        "\n",
        "print(\"Prompt:\",tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
        "output_ids = tf.identity(input_ids)\n",
        "top_n_index, top_n_scores = predict_from_model(model, output_ids)\n",
        "\n",
        "for j in range(n):\n",
        "  score = top_n_scores[0][j].numpy()\n",
        "  next_token = tf.reshape(top_n_index[0][j], [1,-1])\n",
        "  current_output_ids = tf.concat([output_ids,next_token], axis=1)\n",
        "  output_text = tokenizer.decode(current_output_ids[0], skip_special_tokens=True)\n",
        "  print(j,output_text,\":\",score)\n",
        "\n",
        "  # Next word\n",
        "  generate_beam_search_options(1,score, model, current_output_ids)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBM-pS9zbYIU"
      },
      "source": [
        "num_beams = # TODO: set beam width to be 2 \n",
        "max_length = # TODO: generate next 5 words\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "# Using the prebuilt methond in \"transformers\" model\n",
        "outputs = model.generate(\n",
        "    input_ids,  \n",
        "    max_length=max_length, \n",
        "    num_beams=num_beams, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe9wIw-SbYIV"
      },
      "source": [
        "**Challenges with Beam search:**\n",
        "\n",
        "One obvious is the length of generation. As in our example above, the the length of generation is given, but this in some open-ended generations, the length varies or becomes hard to be predicted. Another problem is that distribution of high probability words does not necessarily capture human language. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZI6BiOgje0L"
      },
      "source": [
        "#### <font color=\"orange\">Nano Quiz</font>\n",
        "\n",
        "**2.** When Beam width is set to 1, would Beam search perform the same as greedy search?\n",
        "\n",
        "* A. Yes\n",
        "* B. No"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEZMfjEIje0X"
      },
      "source": [
        "##### <font color=\"purple\">Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sAm5-E23J7Y"
      },
      "source": [
        "A. Yes - Greedy Search can be treated as a special type of beam search with a beam size of 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR1rvF-uchGf"
      },
      "source": [
        "#### <font color=\"orange\">Food for thought\n",
        "\n",
        "* **What is the limiting case of beam search?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qoXArYAmYG0"
      },
      "source": [
        "* Exhaustive Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0VVP0ftbYIX"
      },
      "source": [
        "#### <font color=\"orange\">Text generation using Top-K Sampling</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN_N90GiDqK4"
      },
      "source": [
        "* Greedy search and Beam search are determinstic by probabilities. \n",
        "* We can use sampling to introduce randomness in text generation. \n",
        "* **In Top-K Sampling, the $k$ most likely next words are filtered and the probability mass is redistributed among only those $k$ words.**\n",
        "\n",
        "![Top-K](https://miro.medium.com/max/1400/1*ixvVLan_Ll3MdLDEfJj5qA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySVF-ePTbYIX"
      },
      "source": [
        "top_k = 50\n",
        "\n",
        "output_ids = tf.identity(input_ids)\n",
        "for i in range(5):\n",
        "  model_outputs = model(input_ids=output_ids)\n",
        "  next_token_logits = model_outputs[0][:, -1, :]\n",
        "\n",
        "  # Remove all tokens with a probability less than the last token of the top-k\n",
        "  indices_to_remove = next_token_logits < tf.math.top_k(next_token_logits, k=top_k)[0][..., -1, None]\n",
        "  top_k_logits = tf.zeros_like(next_token_logits) + -float(\"Inf\")\n",
        "  top_k_logits = tf.where(indices_to_remove, top_k_logits, next_token_logits)\n",
        "  \n",
        "  # Next token based on top k probs\n",
        "  next_token = tf.random.categorical(top_k_logits, dtype=tf.int32, num_samples=1)\n",
        "  next_token = tf.reshape(next_token, [1,-1])\n",
        "  output_ids = tf.concat([output_ids,next_token], axis=1)\n",
        "  print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgHvR23EbYIY"
      },
      "source": [
        "# Using the prebuilt methond in \"transformers\" model\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=10, \n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3xsQWaSbYIY"
      },
      "source": [
        "**Top-K sampling is not perfect:** \n",
        "* One concern with Top-K sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution. \n",
        "* This can be problematic as some words might be sampled from a very sharp distribution, whereas others from a much more flat distribution.\n",
        "* Limiting the sample pool to a fixed size K could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfvNK-7rkVHQ"
      },
      "source": [
        "#### <font color=\"orange\">Nano Quiz</font>\n",
        "\n",
        "**3.** In which case we may not want to use top-k sampling for text generation?\n",
        "\n",
        "* A. when the probability distribution is flat \n",
        "* B. when the probability distribution is sharp "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH97AMg-kVHR"
      },
      "source": [
        "##### <font color=\"purple\">Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKWY2H5KkVHR"
      },
      "source": [
        "B. When the probablity distribution is sharp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjD2VS_vbYIY"
      },
      "source": [
        "#### <font color=\"orange\">Text generation using Top-p sampling</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBW5PktZD4-a"
      },
      "source": [
        "In contrast to Top-K sampling, the **Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability $p$.** The probability mass is then redistributed among this set of words. The number of words in such a set can dynamically increase and decrease according to the next word's probability distribution. \n",
        "\n",
        "![Top-p](https://miro.medium.com/max/1400/1*9HEQLJLkPe1Tc1VwIYk5Iw.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-gKqf6ubYIZ"
      },
      "source": [
        "# Using the prebuilt methond in \"transformers\" model\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=10, \n",
        "    top_p=0.80, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu18hpNH97r0"
      },
      "source": [
        "While in theory, Top-p seems more elegant than Top-K, both methods work well in practice. **Top-p can also be used in combination with Top-K, which can avoid very low ranked words while allowing for some dynamic selection.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0MqsfE9sa2Z"
      },
      "source": [
        "#### <font color=\"orange\">Sampling using Temperature parameter</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXTIiEt9Evhw"
      },
      "source": [
        "**Temperature is a hyper-parameter used to control the randomness of predictions by scaling the logits before applying softmax:**\n",
        "\n",
        "* when temperature is a small value (e.g. 0,2), the GPT-2 model is more confident but also more conservative\n",
        "* when temperature is a large value (e.g. 1), the GPT-2 model produces more diversity and also more mistakes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqeBWdz9JkuO"
      },
      "source": [
        "> <font color=\"blue\">**Default Logits i.e. Temperature = 1**\n",
        "\n",
        "![Temp1](https://huggingface.co/blog/assets/02_how-to-generate/sampling_search.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lQ4nsGIJn50"
      },
      "source": [
        "> <font color=\"blue\"> **Temperature = 0.7**\n",
        "\n",
        "![Temp7](https://huggingface.co/blog/assets/02_how-to-generate/sampling_search_with_temp.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUCf_O5zsa2e"
      },
      "source": [
        "# Using the prebuilt method in \"transformers\" model\n",
        "outputs = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=20,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c9QbpX8wXpu"
      },
      "source": [
        "#### <font color=\"orange\">Nano Quiz</font>\n",
        "\n",
        "**4.** As temperature gets closer to zero, the output gets closer to?\n",
        "\n",
        "* A. Greedy Search\n",
        "* B. Beam Search with beam size equal to the size of vocabulary\n",
        "* C. Top-p sampling with p=0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BS4y5ZXwXp3"
      },
      "source": [
        "##### <font color=\"purple\">Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E31x0thnwXp3"
      },
      "source": [
        "A. Greedy Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYd758lKuWpV"
      },
      "source": [
        "#### <font color=\"orange\">Interactive Examples</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EixwQsTIucy2"
      },
      "source": [
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVPHvCd-ypLx"
      },
      "source": [
        "def f(input, max_length, top_p, top_k, temperature, num_beams):\n",
        "\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    input_ids = tokenizer.encode(input, return_tensors='tf')\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids, \n",
        "        do_sample=True, \n",
        "        max_length=max_length, \n",
        "        top_p=top_p, \n",
        "        top_k=top_k,\n",
        "        temperature=temperature,\n",
        "        num_beams=num_beams,\n",
        "    )\n",
        "\n",
        "    print()\n",
        "    print(\"Generated text:\")\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "    print()\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7IZwaguzSRo"
      },
      "source": [
        "interact(\n",
        "    f, \n",
        "    input=\"Today is a great day\",\n",
        "    max_length=widgets.IntSlider(min=10, max=100, step=5, value=10),\n",
        "    top_p=widgets.FloatSlider(min=0.25, max=1, step=0.05, value=0.90), \n",
        "    top_k=widgets.IntSlider(min=1, max=25, step=1, value=10),\n",
        "    temperature=widgets.FloatSlider(min=0.1, max=2, step=0.1, value=1),\n",
        "    num_beams=widgets.IntSlider(min=1, max=5, step=1, value=2),\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHqKaYnbd07_"
      },
      "source": [
        "#### <font color=\"orange\">Parameters for model.generate(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RXMi91Hd08K"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "def generate(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        max_length=None,\n",
        "        min_length=None,\n",
        "        do_sample=None,\n",
        "        early_stopping=None,\n",
        "        num_beams=None,\n",
        "        temperature=None,\n",
        "        top_k=None,\n",
        "        top_p=None,\n",
        "        repetition_penalty=None,\n",
        "        bad_words_ids=None,\n",
        "        bos_token_id=None,\n",
        "        pad_token_id=None,\n",
        "        eos_token_id=None,\n",
        "        length_penalty=None,\n",
        "        no_repeat_ngram_size=None,\n",
        "        num_return_sequences=None,\n",
        "        attention_mask=None,\n",
        "        decoder_start_token_id=None,\n",
        "        use_cache=None,\n",
        "    ):\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "Parameters:\n",
        "        input_ids:\n",
        "            The sequence used as a prompt for the generation.\n",
        "        max_length:\n",
        "            The maximum length of the sequence to be generated.\n",
        "        min_length:\n",
        "            The minimum length of the sequence to be generated.\n",
        "        do_sample:\n",
        "            Whether or not to use sampling ; use greedy decoding otherwise.\n",
        "        early_stopping:\n",
        "            Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
        "        num_beams:\n",
        "            Number of beams for beam search. 1 means no beam search.\n",
        "        temperature:\n",
        "            The value used to module the next token probabilities.\n",
        "        top_k:\n",
        "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "        top_p:\n",
        "            If set to float < 1, only the most probable tokens with probabilities that add up to ``top_p`` or\n",
        "            higher are kept for generation.\n",
        "        repetition_penalty:\n",
        "            The parameter for repetition penalty.\n",
        "        pad_token_id:\n",
        "            The id of the `padding` token.\n",
        "        bos_token_id:\n",
        "            The id of the `beginning-of-sequence` token.\n",
        "        eos_token_id:\n",
        "            The id of the `end-of-sequence` token.\n",
        "        length_penalty:\n",
        "            Exponential penalty to the length. 1.0 means no penalty.    \n",
        "            Set to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in\n",
        "            order to encourage the model to produce longer sequences.\n",
        "        no_repeat_ngram_size:\n",
        "            If set to int > 0, all ngrams of that size can only occur once.\n",
        "        bad_words_ids:\n",
        "            List of token ids that are not allowed to be generated.\n",
        "        num_return_sequences:\n",
        "            The number of independently computed returned sequences for each element in the batch.\n",
        "        attention_mask:\n",
        "            Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for\n",
        "            tokens that are not masked, and 0 for masked tokens.\n",
        "        decoder_start_token_id:\n",
        "            If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
        "        use_cache:\n",
        "            Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
        "            speed up decoding.\n",
        "        model_specific_kwargs:\n",
        "            Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cTC7-8vPT8T"
      },
      "source": [
        "### <font color=\"green\">Additional Notes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNk8c18K9KiI"
      },
      "source": [
        "#### <font color=\"orange\">Understand logits\n",
        "\n",
        "* Logits here means the prediction scores of the language modeling head, namely scores for each vocabulary token before the SoftMax layer.\n",
        "* The shape of logits is \n",
        " ```(batch_size, sequence_length, config.vocab_size) ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDLaGgL09xCl"
      },
      "source": [
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
        "outputs = model(inputs)\n",
        "logits = outputs.logits\n",
        "probs = tf.nn.softmax(logits, axis=-1)\n",
        "print(logits)\n",
        "print(\"==============================\")\n",
        "print(probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ag3md_UF55v"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeR-qb63d8IV"
      },
      "source": [
        "## <font color=\"darkred\">References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcLDIxm0acvb"
      },
      "source": [
        "### <font color=\"green\">Research Papers\n",
        "* [Attention is all you need (2017)](https://arxiv.org/abs/1706.03762)\n",
        "* [Summary of the models](https://huggingface.co/transformers/model_summary.html)\n",
        "* [GPT-2 (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "* [Top-K Sampling](https://arxiv.org/pdf/1805.04833.pdf)\n",
        "* [Top-p Sampling](https://arxiv.org/abs/1904.09751)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx2GENC-d8Ie"
      },
      "source": [
        "### <font color=\"green\">Code\n",
        "\n",
        "- [transformers.generation_tf_utils](https://huggingface.co/transformers/_modules/transformers/generation_tf_utils.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3rKeF1Qd8Ie"
      },
      "source": [
        "### <font color=\"green\">Articles\n",
        "\n",
        "- [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)"
      ]
    }
  ]
}